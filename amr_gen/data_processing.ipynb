{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fake-emo React"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fake emo react dataset we cropped the english samples and made the total dataset size as: - 7012 - real (3793) + fake (3219)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/karish19471/btp/amr'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "train = json.load(open(\"fake-emo_amr/train.json\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def get_s2c(data, sample2comments = {}):\n",
    "    for sample in data:\n",
    "        if sample['idx'] in sample2comments:\n",
    "            sample2comments[sample['idx']]['replies'].append(sample['reply'])\n",
    "        else:\n",
    "            sample2comments[sample['idx']] = {}\n",
    "            sample2comments[sample['idx']]['replies'] = [sample['reply']]\n",
    "            sample2comments[sample['idx']]['label'] = sample['label']\n",
    "            sample2comments[sample['idx']]['text'] = sample['text']\n",
    "    return sample2comments\n",
    "\n",
    "sample2comments = get_s2c(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(sample2comments).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "df = df.set_index(np.arange(len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "real = df[df['label']=='real']\n",
    "fake = df[df['label']=='fake']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3539999/3869150830.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  real['len'] = None\n"
     ]
    }
   ],
   "source": [
    "real['len'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "for i, j in real.iterrows():\n",
    "    if j['replies'][0]!='':\n",
    "        real.at[i, 'len'] = len(j['replies'])\n",
    "    else:\n",
    "        real.at[i, 'len'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "crop_real = real[real['len'] == 1].sample(frac = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "merged  = pd.concat([fake, crop_real])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "del merged['len']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "merged = merged.sample(frac = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "merged = merged.set_index(np.arange(len(merged)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "real    3793\n",
       "fake    3219\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "merged['id'] = \"fake_emo_\"+merged.index.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "merged = merged[['id', 'text', 'replies', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "merged.to_csv(\"fake-emo_amr/fake-emo.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Antivax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "final dataset size is (0, 2865) (1, 932)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/karish19471/btp/amr\n"
     ]
    }
   ],
   "source": [
    "cd btp/amr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import preprocessor as p\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2749\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4293\r"
     ]
    }
   ],
   "source": [
    "antivax = glob.glob(\"antivax_amr/comments/*.json\")\n",
    "len_comments = []\n",
    "df = pd.DataFrame()\n",
    "id_list, comments_list = [], []\n",
    "for lv, news in enumerate(antivax):\n",
    "    all_comments= []\n",
    "    user_comment = json.load(open(news, 'rb'))\n",
    "    for i in user_comment['data']:\n",
    "        all_comments.append(p.clean(i['text']))\n",
    "\n",
    "    if len(all_comments)>0:#considering only news articles with atleast one comment\n",
    "        processed = \"::\".join(all_comments)\n",
    "        comments_list.append(processed)\n",
    "        id_list.append(news[:-1][news[:-1].rfind(\"/\")+1:].split(\".\")[0])\n",
    "    print(lv, end = '\\r', flush = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "df['id'] =  id_list\n",
    "df['comments'] = comments_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1346889653669007361</td>\n",
       "      <td>Read today that they aren't doing the vaccine ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1344313403242852353</td>\n",
       "      <td>Unproven vaccine? Wtf? SOMETHING!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1390453791363903489</td>\n",
       "      <td>At this point it may do just that::Man my arm ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1387777120139718667</td>\n",
       "      <td>I would do it again in a heartbeat!::It was ro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1390040566394540032</td>\n",
       "      <td>G Microchipped Bois Life</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4289</th>\n",
       "      <td>1342225714070888449</td>\n",
       "      <td>Wdym</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4290</th>\n",
       "      <td>1360385133421993987</td>\n",
       "      <td>I'd recommend tapping into some of those sweet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4291</th>\n",
       "      <td>1388666725114122247</td>\n",
       "      <td>Read it again. The study intervention is the t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4292</th>\n",
       "      <td>1372998962328657922</td>\n",
       "      <td>So, the Russians are the Klingons.::So maybe t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4293</th>\n",
       "      <td>1358809886214266884</td>\n",
       "      <td>Goedenavond Joseph</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4294 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       id                                           comments\n",
       "0     1346889653669007361  Read today that they aren't doing the vaccine ...\n",
       "1     1344313403242852353                Unproven vaccine? Wtf? SOMETHING!!!\n",
       "2     1390453791363903489  At this point it may do just that::Man my arm ...\n",
       "3     1387777120139718667  I would do it again in a heartbeat!::It was ro...\n",
       "4     1390040566394540032                           G Microchipped Bois Life\n",
       "...                   ...                                                ...\n",
       "4289  1342225714070888449                                               Wdym\n",
       "4290  1360385133421993987  I'd recommend tapping into some of those sweet...\n",
       "4291  1388666725114122247  Read it again. The study intervention is the t...\n",
       "4292  1372998962328657922  So, the Russians are the Klingons.::So maybe t...\n",
       "4293  1358809886214266884                                 Goedenavond Joseph\n",
       "\n",
       "[4294 rows x 2 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "text = pd.read_csv('antivax_amr/antivax.csv')\n",
    "label = pd.read_csv(\"antivax_amr/Labeled/VaxMisinfoData.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "idx2text  = dict(zip(list(text['idx']), list(text['text'])))\n",
    "idx2label  = dict(zip(list(label['id']), list(label['is_misinfo'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "text, labels = [], []\n",
    "for i, j in df.iterrows():\n",
    "    text.append(idx2text.get(int(j['id']), None))\n",
    "    labels.append(idx2label.get(int(j['id']), None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "df['text'] = text\n",
    "df['labels'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2865\n",
       "1     932\n",
       "Name: labels, dtype: int64"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['labels'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "df = df.rename(columns = {'id':'tweet_id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "df['id'] = \"antivax_\"+df.index.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "df = df[['id', 'tweet_id', 'text', 'comments', 'labels']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"antivax_amr/antivax.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>comments</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>antivax_1</td>\n",
       "      <td>1344313403242852353</td>\n",
       "      <td>They are doing this, so you beg for an unprove...</td>\n",
       "      <td>Unproven vaccine? Wtf? SOMETHING!!!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>antivax_2</td>\n",
       "      <td>1390453791363903489</td>\n",
       "      <td>Fully vaccinated, lets see if the 2nd dose “va...</td>\n",
       "      <td>At this point it may do just that::Man my arm ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>antivax_3</td>\n",
       "      <td>1387777120139718667</td>\n",
       "      <td>Big BIG feels #vaccinated #thankyouscience #Mo...</td>\n",
       "      <td>I would do it again in a heartbeat!::It was ro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>antivax_4</td>\n",
       "      <td>1390040566394540032</td>\n",
       "      <td>.@ZaddiHen grateful for the vaccine link. Neve...</td>\n",
       "      <td>G Microchipped Bois Life</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>antivax_5</td>\n",
       "      <td>1359290042821341184</td>\n",
       "      <td>just got an email about getting my covid vacci...</td>\n",
       "      <td>lol JK i have to wait months to get it now! yo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4289</th>\n",
       "      <td>antivax_4289</td>\n",
       "      <td>1342225714070888449</td>\n",
       "      <td>If you drink redbull don’t worry about what’s ...</td>\n",
       "      <td>Wdym</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4290</th>\n",
       "      <td>antivax_4290</td>\n",
       "      <td>1360385133421993987</td>\n",
       "      <td>Just got my first vaccine dose. One step close...</td>\n",
       "      <td>I'd recommend tapping into some of those sweet...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4291</th>\n",
       "      <td>antivax_4291</td>\n",
       "      <td>1388666725114122247</td>\n",
       "      <td>See page 67 - 69 on possible ‘shedding’ // vac...</td>\n",
       "      <td>Read it again. The study intervention is the t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4292</th>\n",
       "      <td>antivax_4292</td>\n",
       "      <td>1372998962328657922</td>\n",
       "      <td>If you threaten to poison vaccines because som...</td>\n",
       "      <td>So, the Russians are the Klingons.::So maybe t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4293</th>\n",
       "      <td>antivax_4293</td>\n",
       "      <td>1358809886214266884</td>\n",
       "      <td>CDC: Over 500 Deaths Now Following mRNA Experi...</td>\n",
       "      <td>Goedenavond Joseph</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3797 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                id             tweet_id  \\\n",
       "1        antivax_1  1344313403242852353   \n",
       "2        antivax_2  1390453791363903489   \n",
       "3        antivax_3  1387777120139718667   \n",
       "4        antivax_4  1390040566394540032   \n",
       "5        antivax_5  1359290042821341184   \n",
       "...            ...                  ...   \n",
       "4289  antivax_4289  1342225714070888449   \n",
       "4290  antivax_4290  1360385133421993987   \n",
       "4291  antivax_4291  1388666725114122247   \n",
       "4292  antivax_4292  1372998962328657922   \n",
       "4293  antivax_4293  1358809886214266884   \n",
       "\n",
       "                                                   text  \\\n",
       "1     They are doing this, so you beg for an unprove...   \n",
       "2     Fully vaccinated, lets see if the 2nd dose “va...   \n",
       "3     Big BIG feels #vaccinated #thankyouscience #Mo...   \n",
       "4     .@ZaddiHen grateful for the vaccine link. Neve...   \n",
       "5     just got an email about getting my covid vacci...   \n",
       "...                                                 ...   \n",
       "4289  If you drink redbull don’t worry about what’s ...   \n",
       "4290  Just got my first vaccine dose. One step close...   \n",
       "4291  See page 67 - 69 on possible ‘shedding’ // vac...   \n",
       "4292  If you threaten to poison vaccines because som...   \n",
       "4293  CDC: Over 500 Deaths Now Following mRNA Experi...   \n",
       "\n",
       "                                               comments  labels  \n",
       "1                   Unproven vaccine? Wtf? SOMETHING!!!       1  \n",
       "2     At this point it may do just that::Man my arm ...       0  \n",
       "3     I would do it again in a heartbeat!::It was ro...       0  \n",
       "4                              G Microchipped Bois Life       0  \n",
       "5     lol JK i have to wait months to get it now! yo...       0  \n",
       "...                                                 ...     ...  \n",
       "4289                                               Wdym       0  \n",
       "4290  I'd recommend tapping into some of those sweet...       0  \n",
       "4291  Read it again. The study intervention is the t...       1  \n",
       "4292  So, the Russians are the Klingons.::So maybe t...       1  \n",
       "4293                                 Goedenavond Joseph       1  \n",
       "\n",
       "[3797 rows x 5 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# figlang_reddit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOT_SARCASM    2200\n",
    "SARCASM        2200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd    \n",
    "reddit = pd.read_json(path_or_buf=\"figlang_reddit_amr/sarcasm_detection_shared_task_reddit_training.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "reddit = reddit.rename(columns={'response':'text', 'context':'comments'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "lens = np.array([len(i) for i in list(reddit['comments'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "reddit['id'] = \"reddit_\"+reddit.index.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>comments</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SARCASM</td>\n",
       "      <td>Yeah I mean there's only one gender anyways, w...</td>\n",
       "      <td>[LPT: If you're worried about hurting someone'...</td>\n",
       "      <td>reddit_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SARCASM</td>\n",
       "      <td>Sounds like you don't like science, you theist...</td>\n",
       "      <td>[Promotional images for some guy's Facebook pa...</td>\n",
       "      <td>reddit_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SARCASM</td>\n",
       "      <td>Ofc play them in try mode, Blizzard were so ge...</td>\n",
       "      <td>[My friends won't play Dota2; I won't play LoL...</td>\n",
       "      <td>reddit_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SARCASM</td>\n",
       "      <td>I don't understand, Reddit told me that Hillar...</td>\n",
       "      <td>[Poll: Convention boosts Clinton to 11-point l...</td>\n",
       "      <td>reddit_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SARCASM</td>\n",
       "      <td>yeh, they're the reigning triple premiers, why...</td>\n",
       "      <td>[Wayne Ludbey: Jordan Lewis has the ultimate c...</td>\n",
       "      <td>reddit_4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4395</th>\n",
       "      <td>NOT_SARCASM</td>\n",
       "      <td>well you could've been adulting if you hadn't ...</td>\n",
       "      <td>[8-9ft man found in ancient indian burial moun...</td>\n",
       "      <td>reddit_4395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4396</th>\n",
       "      <td>NOT_SARCASM</td>\n",
       "      <td>Also they'll have to join the euro</td>\n",
       "      <td>[Second Scottish independence referendum 'on t...</td>\n",
       "      <td>reddit_4396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4397</th>\n",
       "      <td>NOT_SARCASM</td>\n",
       "      <td>plot: AI assists a cyborg in freelance investi...</td>\n",
       "      <td>[Pinoy Cyborg by James Simmons, Mag-ingat sa r...</td>\n",
       "      <td>reddit_4397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4398</th>\n",
       "      <td>NOT_SARCASM</td>\n",
       "      <td>Some airlines proposed this but too much publi...</td>\n",
       "      <td>[The logic here is flawless!, No it isn't, for...</td>\n",
       "      <td>reddit_4398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4399</th>\n",
       "      <td>NOT_SARCASM</td>\n",
       "      <td>Any number of corporate shill organizations ba...</td>\n",
       "      <td>[TIL One of the founding members of Greenpeace...</td>\n",
       "      <td>reddit_4399</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4400 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            label                                               text  \\\n",
       "0         SARCASM  Yeah I mean there's only one gender anyways, w...   \n",
       "1         SARCASM  Sounds like you don't like science, you theist...   \n",
       "2         SARCASM  Ofc play them in try mode, Blizzard were so ge...   \n",
       "3         SARCASM  I don't understand, Reddit told me that Hillar...   \n",
       "4         SARCASM  yeh, they're the reigning triple premiers, why...   \n",
       "...           ...                                                ...   \n",
       "4395  NOT_SARCASM  well you could've been adulting if you hadn't ...   \n",
       "4396  NOT_SARCASM                 Also they'll have to join the euro   \n",
       "4397  NOT_SARCASM  plot: AI assists a cyborg in freelance investi...   \n",
       "4398  NOT_SARCASM  Some airlines proposed this but too much publi...   \n",
       "4399  NOT_SARCASM  Any number of corporate shill organizations ba...   \n",
       "\n",
       "                                               comments           id  \n",
       "0     [LPT: If you're worried about hurting someone'...     reddit_0  \n",
       "1     [Promotional images for some guy's Facebook pa...     reddit_1  \n",
       "2     [My friends won't play Dota2; I won't play LoL...     reddit_2  \n",
       "3     [Poll: Convention boosts Clinton to 11-point l...     reddit_3  \n",
       "4     [Wayne Ludbey: Jordan Lewis has the ultimate c...     reddit_4  \n",
       "...                                                 ...          ...  \n",
       "4395  [8-9ft man found in ancient indian burial moun...  reddit_4395  \n",
       "4396  [Second Scottish independence referendum 'on t...  reddit_4396  \n",
       "4397  [Pinoy Cyborg by James Simmons, Mag-ingat sa r...  reddit_4397  \n",
       "4398  [The logic here is flawless!, No it isn't, for...  reddit_4398  \n",
       "4399  [TIL One of the founding members of Greenpeace...  reddit_4399  \n",
       "\n",
       "[4400 rows x 4 columns]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "reddit = reddit[['id', 'text', 'comments', 'label']].sample(frac = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NOT_SARCASM    2200\n",
       "SARCASM        2200\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "reddit.to_csv(\"figlang_reddit/reddit.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# figlang_twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import preprocessor as pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd    \n",
    "twitter = pd.read_json(path_or_buf=\"figlang_twitter_amr/sarcasm_detection_shared_task_twitter_training.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "twitter = twitter.rename(columns={'response':'text', 'context':'comments'})\n",
    "twitter['id'] = \"twitter_\"+twitter.index.astype(str)\n",
    "twitter = twitter[['id', 'text', 'comments', 'label']].sample(frac = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "for i, j in twitter.iterrows():\n",
    "    twitter.at[i, 'text'] = pro.clean(j['text'])\n",
    "    comm = j['comments']\n",
    "    cleaned = []\n",
    "    for o in comm:\n",
    "        cleaned.append(pro.clean(o).strip())\n",
    "    twitter.at[i, 'comments'] = \"::\".join(cleaned)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "twitter.to_csv(\"figlang_twitter_amr/figlang_twitter.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hasoc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NONE    414\n",
    "HOF     298"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "dirs = glob.glob(\"/home/karish19471/btp/amr/hasoc_amr/Data/task2_data/train/*/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import preprocessor as p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "data_comments = []\n",
    "data_text = []\n",
    "data_id = []\n",
    "id2label = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "for directory in dirs:\n",
    "    tweets = glob.glob(f'{directory}*/')\n",
    "    for tweet in tweets:\n",
    "        data = json.load(open(f'{tweet}data.json', 'rb'))\n",
    "        LABEL = json.load(open(f'{tweet}labels.json', 'rb'))\n",
    "        for key in LABEL:\n",
    "            id2label[key] = LABEL[key]\n",
    "        # data = json.load(open(f'/home/karish19471/btp/amr/hasoc/Data/task2_data/train/bantwitter/1397101600460529665/data.json', 'rb'))\n",
    "        text = p.clean(data['tweet'])\n",
    "        tweet_id = data['tweet_id']\n",
    "        extra_tweets_text = []\n",
    "        extra_tweets_replies = []\n",
    "        extra_tweets_ids = []\n",
    "        comments = []\n",
    "        for comment in data['comments']:\n",
    "            tmp = comment.get('replies', -1)\n",
    "            if tmp==-1:\n",
    "                comments.append(p.clean(comment['tweet']))\n",
    "            else:\n",
    "                comments.append(p.clean(comment['tweet']))\n",
    "                \n",
    "                replies_list = []\n",
    "                for replies in tmp:\n",
    "                    comments.append(p.clean(replies['tweet']))\n",
    "                    replies_list.append(p.clean(replies['tweet']))\n",
    "                \n",
    "                extra_tweets_text.append(p.clean(comment['tweet']))\n",
    "                extra_tweets_ids.append(comment['tweet_id'])\n",
    "                extra_tweets_replies.append(\"::\".join(replies_list))\n",
    "\n",
    "        extra_tweets_replies.append(\"::\".join(comments))\n",
    "        extra_tweets_ids.append(tweet_id)\n",
    "        extra_tweets_text.append(text)\n",
    "\n",
    "        data_comments.extend(extra_tweets_replies)\n",
    "        data_text.extend(extra_tweets_text)\n",
    "        data_id.extend(extra_tweets_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(712, 712, 712)"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_id), len(data_text), len(data_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "hasoc = pd.DataFrame()\n",
    "hasoc['twitter_id'] = data_id\n",
    "hasoc['text'] = data_text\n",
    "hasoc['comments'] = data_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "hasoc['label'] = None\n",
    "for i, j in hasoc.iterrows():\n",
    "    j['label'] = id2label[str(j['twitter_id'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NONE    414\n",
       "HOF     298\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hasoc['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>twitter_id</th>\n",
       "      <th>text</th>\n",
       "      <th>comments</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1395727658323898368</td>\n",
       "      <td>Point</td>\n",
       "      <td>No*</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1395695944583180293</td>\n",
       "      <td>Yes terrorism from Pakistan as well as RSS. I ...</td>\n",
       "      <td>Terrorism from lefrist, missionaries and jihad...</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1395703843162267648</td>\n",
       "      <td>Accha! Climate Change to gaya, khatam!!</td>\n",
       "      <td>?</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1395714606278975492</td>\n",
       "      <td></td>\n",
       "      <td>?</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1395742992019820552</td>\n",
       "      <td>Ab panditon ki dukaan bnd or paakhnd bhi bnd</td>\n",
       "      <td>Great I am waiting for the day.::Isaaio se jya...</td>\n",
       "      <td>HOF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>707</th>\n",
       "      <td>1397949847609614342</td>\n",
       "      <td>India was able to screw Huawei when it came to...</td>\n",
       "      <td>Absolutely.. no nd thought on this issue::Plea...</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>708</th>\n",
       "      <td>1397979516585213953</td>\n",
       "      <td>So as a matter of protest please quit twitter.</td>\n",
       "      <td>Its a same logic pakistan niklo ..Why u want t...</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709</th>\n",
       "      <td>1397999669607686145</td>\n",
       "      <td>Sir, Twitter is the new East India Company, bu...</td>\n",
       "      <td>Simple...You can deactivate your acc in one cl...</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>710</th>\n",
       "      <td>1397914911431217152</td>\n",
       "      <td>Twitter is the new East India Companypretendin...</td>\n",
       "      <td>Twitter has been acting like a dictator and ba...</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>711</th>\n",
       "      <td>1397959669696634885</td>\n",
       "      <td>Twitter urges Indian government to respect fre...</td>\n",
       "      <td>This has to be from the Onion.::let's ask this...</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>712 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              twitter_id                                               text  \\\n",
       "0    1395727658323898368                                              Point   \n",
       "1    1395695944583180293  Yes terrorism from Pakistan as well as RSS. I ...   \n",
       "2    1395703843162267648            Accha! Climate Change to gaya, khatam!!   \n",
       "3    1395714606278975492                                                      \n",
       "4    1395742992019820552       Ab panditon ki dukaan bnd or paakhnd bhi bnd   \n",
       "..                   ...                                                ...   \n",
       "707  1397949847609614342  India was able to screw Huawei when it came to...   \n",
       "708  1397979516585213953     So as a matter of protest please quit twitter.   \n",
       "709  1397999669607686145  Sir, Twitter is the new East India Company, bu...   \n",
       "710  1397914911431217152  Twitter is the new East India Companypretendin...   \n",
       "711  1397959669696634885  Twitter urges Indian government to respect fre...   \n",
       "\n",
       "                                              comments label  \n",
       "0                                                  No*  NONE  \n",
       "1    Terrorism from lefrist, missionaries and jihad...  NONE  \n",
       "2                                                    ?  NONE  \n",
       "3                                                    ?  NONE  \n",
       "4    Great I am waiting for the day.::Isaaio se jya...   HOF  \n",
       "..                                                 ...   ...  \n",
       "707  Absolutely.. no nd thought on this issue::Plea...  NONE  \n",
       "708  Its a same logic pakistan niklo ..Why u want t...  NONE  \n",
       "709  Simple...You can deactivate your acc in one cl...  NONE  \n",
       "710  Twitter has been acting like a dictator and ba...  NONE  \n",
       "711  This has to be from the Onion.::let's ask this...  NONE  \n",
       "\n",
       "[712 rows x 4 columns]"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hasoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "hasoc['id'] = \"hasoc_\"+hasoc.index.astype(str)\n",
    "hasoc = hasoc[['id','twitter_id', 'text', 'comments', 'label']].sample(frac = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "hasoc.to_csv(\"hasoc/hasoc.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# jing_gab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NON_HATE    7665\n",
    "HATE        7665"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "gab = pd.read_csv(\"jing_gab_amr/gab.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "gab = gab.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import preprocessor as pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "hate_texts, hate_context, non_hate_texts, non_hate_context = [], [], [], []\n",
    "for i, j in gab.iterrows():\n",
    "    j['text'] = j['text'].replace('\\t', '')\n",
    "    texts = j['text'].strip().split('\\n')\n",
    "    new_texts = []\n",
    "    for p in texts:\n",
    "        new_texts.append(pro.clean(\".\".join(p.split(\".\")[1:]).strip()))\n",
    "    texts = new_texts\n",
    "    idx = eval(j['hate_speech_idx']) \n",
    "\n",
    "    h_texts  = [v for i, v in enumerate(texts) if i+1 in idx]\n",
    "    n_h_texts = [v for i, v in enumerate(texts) if i+1 not in idx]\n",
    "\n",
    "    try:\n",
    "        source_n_h_text = n_h_texts[0]\n",
    "        rest_n_h_text = n_h_texts[1:]\n",
    "\n",
    "        non_hate_texts.append(source_n_h_text)\n",
    "        non_hate_context.append(\"::\".join(rest_n_h_text))\n",
    "\n",
    "        hate_texts.append(h_texts[0])\n",
    "        rest_h_text = n_h_texts + eval(j['response'])\n",
    "\n",
    "        hate_context.append(\"::\".join(rest_h_text))\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "gab_final = pd.DataFrame()\n",
    "gab_final['text'] = hate_texts + non_hate_texts\n",
    "gab_final['comments'] = hate_context + non_hate_context\n",
    "gab_final['labels'] = [\"HATE\"]*len(hate_context) + [\"NON_HATE\"]*len(non_hate_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "gab_final = gab_final.sample(frac= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "gab_final.to_csv(\"jing_gab_amr/gab.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NON_HATE    7665\n",
       "HATE        7665\n",
       "Name: labels, dtype: int64"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gab_final['labels'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# jing_twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NON_HATE    2490\n",
    "HATE        2488"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "reddit = pd.read_csv(\"jing_reddit_amr/reddit.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "reddit = reddit.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "hate_texts, hate_context, non_hate_texts, non_hate_context = [], [], [], []\n",
    "for i, j in reddit.iterrows():\n",
    "    j['text'] = j['text'].replace('\\t', '')\n",
    "    texts = j['text'].strip().split('\\n')\n",
    "    new_texts = []\n",
    "    for p in texts:\n",
    "        new_texts.append(pro.clean(\".\".join(p.split(\".\")[1:]).strip()))\n",
    "    texts = new_texts\n",
    "    idx = eval(j['hate_speech_idx']) \n",
    "\n",
    "    h_texts  = [v for i, v in enumerate(texts) if i+1 in idx]\n",
    "    n_h_texts = [v for i, v in enumerate(texts) if i+1 not in idx]\n",
    "\n",
    "    try:\n",
    "        source_n_h_text = n_h_texts[0]\n",
    "        rest_n_h_text = n_h_texts[1:]\n",
    "\n",
    "        non_hate_texts.append(source_n_h_text)\n",
    "        non_hate_context.append(\"::\".join(rest_n_h_text))\n",
    "\n",
    "        hate_texts.append(h_texts[0])\n",
    "        rest_h_text = n_h_texts + eval(j['response'])\n",
    "\n",
    "        hate_context.append(\"::\".join(rest_h_text))\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "reddit_final = pd.DataFrame()\n",
    "reddit_final['text'] = hate_texts + non_hate_texts\n",
    "reddit_final['comments'] = hate_context + non_hate_context\n",
    "reddit_final['labels'] = [\"HATE\"]*len(hate_context) + [\"NON_HATE\"]*len(non_hate_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "reddit_final = reddit_final.sample(frac= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NON_HATE    2490\n",
       "HATE        2488\n",
       "Name: labels, dtype: int64"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_final['labels'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "reddit_final.to_csv(\"jing_reddit_amr/reddit.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# twiiter16 and twitter15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import preprocessor as p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "542\r"
     ]
    }
   ],
   "source": [
    "twitter = glob.glob(\"twitter15_amr/comments/*.json\")\n",
    "len_comments = []\n",
    "df = pd.DataFrame()\n",
    "id_list, comments_list = [], []\n",
    "for lv, news in enumerate(twitter):\n",
    "    all_comments= []\n",
    "    user_comment = json.load(open(news, 'rb'))\n",
    "    for i in user_comment['data']:\n",
    "        all_comments.append(p.clean(i['text']))\n",
    "\n",
    "    if len(all_comments)>0:#considering only news articles with atleast one comment\n",
    "        processed = \"::\".join(all_comments)\n",
    "        comments_list.append(processed)\n",
    "        id_list.append(news[:-1][news[:-1].rfind(\"/\")+1:].split(\".\")[0])\n",
    "    print(lv, end = '\\r', flush = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "df['id'] =  id_list\n",
    "df['comments'] = comments_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "label = pd.read_csv(\"twitter15_amr/label.txt\", sep = ':')\n",
    "text = pd.read_csv(\"twitter15_amr/source_tweets.txt\", sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "label.columns = ['label', 'id']\n",
    "text.columns = ['id', 'text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "id2label = dict(list(zip(label['id'], label['label'])))\n",
    "id2text = dict(list(zip(text['id'], text['text'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "df['label'] = None\n",
    "df['text'] = None\n",
    "for i, j in df.iterrows():\n",
    "    j['label'] = id2label.get(int(j['id']), None)\n",
    "    j['text'] = p.clean(id2text.get(int(j['id']), None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "df = df[['id', 'text', 'comments', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"twitter15_amr/twitter15.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>comments</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>427944719612915712</td>\n",
       "      <td>ex-marlboro man dies from smoking-related dise...</td>\n",
       "      <td>: Ex-Marlboro man dies from smoking-related di...</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>553264892799488000</td>\n",
       "      <td>three more passport stamps, three more data ce...</td>\n",
       "      <td>e::Go to confirm that you re + enter email mak...</td>\n",
       "      <td>false</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>531108688749404160</td>\n",
       "      <td>news that 'home alone' star macauley culkin is...</td>\n",
       "      <td>Hahahahahahaha</td>\n",
       "      <td>false</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>524928878075457536</td>\n",
       "      <td>soldier shot at national war memorial in ottaw...</td>\n",
       "      <td>Just Hurd very heart breaking !! My heart goes...</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>531565935128498176</td>\n",
       "      <td>hey, it looks like... oh. rt : photo of vladim...</td>\n",
       "      <td>Yeah ...eat and not freeze to death .... what ...</td>\n",
       "      <td>false</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>538</th>\n",
       "      <td>377519445578895360</td>\n",
       "      <td>ahh!! has an encyclopedia in the works for !! ...</td>\n",
       "      <td>FINALLY WOOOWOWOO::fhjdjdjkdkodopglg::fangirl:...</td>\n",
       "      <td>false</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>539</th>\n",
       "      <td>535315394127728641</td>\n",
       "      <td>raven symone blasts bill cosby rape story URL URL</td>\n",
       "      <td>I thought that when something like that happen...</td>\n",
       "      <td>false</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>540</th>\n",
       "      <td>489841070818488320</td>\n",
       "      <td>'nine britons and children' feared dead after ...</td>\n",
       "      <td>Of course. Ppl can detach fm that which is hap...</td>\n",
       "      <td>false</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>541</th>\n",
       "      <td>510918747188514816</td>\n",
       "      <td>breaking: video claims to show beheading of br...</td>\n",
       "      <td>Is that so they have free reign and can kill i...</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>542</th>\n",
       "      <td>524312881823637504</td>\n",
       "      <td>google south american goliath birdeater. helll...</td>\n",
       "      <td>::Did you watch the video on YouTube? It's rea...</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>543 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id                                               text  \\\n",
       "0    427944719612915712  ex-marlboro man dies from smoking-related dise...   \n",
       "1    553264892799488000  three more passport stamps, three more data ce...   \n",
       "2    531108688749404160  news that 'home alone' star macauley culkin is...   \n",
       "3    524928878075457536  soldier shot at national war memorial in ottaw...   \n",
       "4    531565935128498176  hey, it looks like... oh. rt : photo of vladim...   \n",
       "..                  ...                                                ...   \n",
       "538  377519445578895360  ahh!! has an encyclopedia in the works for !! ...   \n",
       "539  535315394127728641  raven symone blasts bill cosby rape story URL URL   \n",
       "540  489841070818488320  'nine britons and children' feared dead after ...   \n",
       "541  510918747188514816  breaking: video claims to show beheading of br...   \n",
       "542  524312881823637504  google south american goliath birdeater. helll...   \n",
       "\n",
       "                                              comments  label  \n",
       "0    : Ex-Marlboro man dies from smoking-related di...   true  \n",
       "1    e::Go to confirm that you re + enter email mak...  false  \n",
       "2                                       Hahahahahahaha  false  \n",
       "3    Just Hurd very heart breaking !! My heart goes...   true  \n",
       "4    Yeah ...eat and not freeze to death .... what ...  false  \n",
       "..                                                 ...    ...  \n",
       "538  FINALLY WOOOWOWOO::fhjdjdjkdkodopglg::fangirl:...  false  \n",
       "539  I thought that when something like that happen...  false  \n",
       "540  Of course. Ppl can detach fm that which is hap...  false  \n",
       "541  Is that so they have free reign and can kill i...   true  \n",
       "542  ::Did you watch the video on YouTube? It's rea...   true  \n",
       "\n",
       "[543 rows x 4 columns]"
      ]
     },
     "execution_count": 468,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pheme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "non_rumour    4023\n",
    "rumour        2402\n",
    "Name: labels, dtype: int64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import glob\n",
    "import pandas as pd\n",
    "import preprocessor as pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "non_rumours = glob.glob(\"pheme_amr/all-rnr-annotated-threads/*/non-rumours/*/\")\n",
    "rumours = glob.glob(\"pheme_amr/all-rnr-annotated-threads/*/rumours/*/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "all_rumours = []\n",
    "all_source_rumours = []\n",
    "all_comments_rumours = []\n",
    "for r in rumours:\n",
    "    name = r[:-1][r[:-1].rfind(\"/\")+1:]\n",
    "    comments = glob.glob(r+\"reactions/*.json\")\n",
    "    comment_list = []\n",
    "    for c in comments:\n",
    "        j_file = json.load(open(c, 'rb'))\n",
    "        comment_list.append(pro.clean(j_file['text']))\n",
    "    combined = \"::\".join(comment_list)\n",
    "    all_comments_rumours.append(combined)\n",
    "    all_rumours.append(r)\n",
    "    all_source_rumours.append(pro.clean(json.load(open(r+f\"source-tweets/{name}.json\", 'rb'))['text']))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "all_non_rumours = []\n",
    "all_non_source_rumours = []\n",
    "all_non_comments_rumours = []\n",
    "for r in non_rumours:\n",
    "    name = r[:-1][r[:-1].rfind(\"/\")+1:]\n",
    "    comments = glob.glob(r+\"reactions/*.json\")\n",
    "    comment_list = []\n",
    "    for c in comments:\n",
    "        j_file = json.load(open(c, 'rb'))\n",
    "        comment_list.append(pro.clean(j_file['text']))\n",
    "    combined = \"::\".join(comment_list)\n",
    "    all_non_comments_rumours.append(combined)\n",
    "    all_non_rumours.append(r)\n",
    "    all_non_source_rumours.append(pro.clean(json.load(open(r+f\"source-tweets/{name}.json\", 'rb'))['text']))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df['text'] = all_source_rumours + all_non_source_rumours\n",
    "df['comments'] = all_comments_rumours + all_non_comments_rumours\n",
    "df['labels'] = ['rumour']*len(all_rumours) + ['non_rumour']*len(all_non_rumours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "df = df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "non_rumour    4023\n",
       "rumour        2402\n",
       "Name: labels, dtype: int64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['labels'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "del df['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "df['id'] = \"pheme_\"+df.index.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "df = df[['id', 'text', 'comments', 'labels']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"pheme_amr/pheme.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>comments</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4368</th>\n",
       "      <td>pheme_4368</td>\n",
       "      <td>Just fyi, if you're using a hostage situation ...</td>\n",
       "      <td>It was a pew research poll on all news channel...</td>\n",
       "      <td>non_rumour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5645</th>\n",
       "      <td>pheme_5645</td>\n",
       "      <td>New cops wear blue not camo, mingle and talk t...</td>\n",
       "      <td>really think getting control of policing out o...</td>\n",
       "      <td>non_rumour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>544</th>\n",
       "      <td>pheme_544</td>\n",
       "      <td>One pilot in crash was locked out of cockpit, ...</td>\n",
       "      <td>Cockpit! is that the same problem Air France h...</td>\n",
       "      <td>rumour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>pheme_499</td>\n",
       "      <td>Killed Charlie Hebdo suspects came out firing ...</td>\n",
       "      <td>I dont no::Got killed::: Killed Charlie Hebdo ...</td>\n",
       "      <td>rumour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4852</th>\n",
       "      <td>pheme_4852</td>\n",
       "      <td>charges of sexual assault against Sydney hosta...</td>\n",
       "      <td>cops storming the place right now::sex assault...</td>\n",
       "      <td>non_rumour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5852</th>\n",
       "      <td>pheme_5852</td>\n",
       "      <td>Also, the National Association of Black Psycho...</td>\n",
       "      <td></td>\n",
       "      <td>non_rumour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1700</th>\n",
       "      <td>pheme_1700</td>\n",
       "      <td>The other day I asked a manager at the Market ...</td>\n",
       "      <td>thanks. Now we need a pic of what michael brow...</td>\n",
       "      <td>rumour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2597</th>\n",
       "      <td>pheme_2597</td>\n",
       "      <td>Love will win from hatred</td>\n",
       "      <td>. Bro! Twitter \"Verify\" ?!::\" \"::lol::\" \" \" ?\"...</td>\n",
       "      <td>non_rumour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>pheme_217</td>\n",
       "      <td>MORE: Police surround kosher grocery store in ...</td>\n",
       "      <td>*sigh* smh MORE: Police surround kosher grocer...</td>\n",
       "      <td>rumour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>845</th>\n",
       "      <td>pheme_845</td>\n",
       "      <td>Here's a picture of a stage set up right now a...</td>\n",
       "      <td></td>\n",
       "      <td>rumour</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6425 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id                                               text  \\\n",
       "4368  pheme_4368  Just fyi, if you're using a hostage situation ...   \n",
       "5645  pheme_5645  New cops wear blue not camo, mingle and talk t...   \n",
       "544    pheme_544  One pilot in crash was locked out of cockpit, ...   \n",
       "499    pheme_499  Killed Charlie Hebdo suspects came out firing ...   \n",
       "4852  pheme_4852  charges of sexual assault against Sydney hosta...   \n",
       "...          ...                                                ...   \n",
       "5852  pheme_5852  Also, the National Association of Black Psycho...   \n",
       "1700  pheme_1700  The other day I asked a manager at the Market ...   \n",
       "2597  pheme_2597                          Love will win from hatred   \n",
       "217    pheme_217  MORE: Police surround kosher grocery store in ...   \n",
       "845    pheme_845  Here's a picture of a stage set up right now a...   \n",
       "\n",
       "                                               comments      labels  \n",
       "4368  It was a pew research poll on all news channel...  non_rumour  \n",
       "5645  really think getting control of policing out o...  non_rumour  \n",
       "544   Cockpit! is that the same problem Air France h...      rumour  \n",
       "499   I dont no::Got killed::: Killed Charlie Hebdo ...      rumour  \n",
       "4852  cops storming the place right now::sex assault...  non_rumour  \n",
       "...                                                 ...         ...  \n",
       "5852                                                     non_rumour  \n",
       "1700  thanks. Now we need a pic of what michael brow...      rumour  \n",
       "2597  . Bro! Twitter \"Verify\" ?!::\" \"::lol::\" \" \" ?\"...  non_rumour  \n",
       "217   *sigh* smh MORE: Police surround kosher grocer...      rumour  \n",
       "845                                                          rumour  \n",
       "\n",
       "[6425 rows x 4 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# rumoureval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "train = json.load(open(\"rumoureval_amr/train.json\", 'rb'))\n",
    "test = json.load(open(\"rumoureval_amr/test.json\", 'rb'))\n",
    "val = json.load(open(\"rumoureval_amr/val.json\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "reval = train + test + val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "all_comments = []\n",
    "all_text = []\n",
    "all_labels = []\n",
    "for sample in reval:\n",
    "    all_comments.append(\"::\".join([pro.clean(i) for i in sample['comments']]))\n",
    "    all_labels.append(sample['label'])\n",
    "    all_text.append(pro.clean(sample['content']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df['text'] = all_text\n",
    "df['labels'] = all_labels\n",
    "df['comments'] = all_comments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"rumoureval_amr/rumoureval.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# convert all datasets to same format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"fake-emo_amr/fake-emo.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "df = df.rename(columns={'replies':'comments'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "for i, j in df.iterrows():\n",
    "    df.at[i, 'comments'] = \"::\".join(eval(j['comments']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"figlang_twitter_amr/figlang_twitter.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "415\n",
      "2813\n"
     ]
    }
   ],
   "source": [
    "datasets = ['politifact', 'gossipcop']\n",
    "for dataset in datasets:\n",
    "    df = pd.read_csv(f\"{dataset}_amr/{dataset}.tsv\", sep = '\\t')\n",
    "    print(df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "antivax 3797 \n",
      " 0    2865\n",
      "1     932\n",
      "Name: labels, dtype: int64\n",
      "\n",
      "*****************************\n",
      "fake-emo 7012 \n",
      " real    3793\n",
      "fake    3219\n",
      "Name: labels, dtype: int64\n",
      "\n",
      "*****************************\n",
      "jing_gab 15330 \n",
      " NON_HATE    7665\n",
      "HATE        7665\n",
      "Name: labels, dtype: int64\n",
      "\n",
      "*****************************\n",
      "jing_reddit 4978 \n",
      " NON_HATE    2490\n",
      "HATE        2488\n",
      "Name: labels, dtype: int64\n",
      "\n",
      "*****************************\n",
      "figlang_reddit 4400 \n",
      " NOT_SARCASM    2200\n",
      "SARCASM        2200\n",
      "Name: labels, dtype: int64\n",
      "\n",
      "*****************************\n",
      "figlang_twitter 4400 \n",
      " NOT_SARCASM    2200\n",
      "SARCASM        2200\n",
      "Name: labels, dtype: int64\n",
      "\n",
      "*****************************\n",
      "hasoc 712 \n",
      " NONE    414\n",
      "HOF     298\n",
      "Name: labels, dtype: int64\n",
      "\n",
      "*****************************\n",
      "pheme 6425 \n",
      " non_rumour    4023\n",
      "rumour        2402\n",
      "Name: labels, dtype: int64\n",
      "\n",
      "*****************************\n",
      "twitter15 543 \n",
      " True     276\n",
      "False    267\n",
      "Name: labels, dtype: int64\n",
      "\n",
      "*****************************\n",
      "twitter16 362 \n",
      " False    199\n",
      "True     163\n",
      "Name: labels, dtype: int64\n",
      "\n",
      "*****************************\n",
      "rumoureval 446 \n",
      " real          185\n",
      "fake          138\n",
      "unverified    123\n",
      "Name: labels, dtype: int64\n",
      "\n",
      "*****************************\n"
     ]
    }
   ],
   "source": [
    "datasets = ['antivax', 'fake-emo', 'jing_gab', 'jing_reddit', 'figlang_reddit', 'figlang_twitter','hasoc', 'pheme', 'twitter15', 'twitter16', 'rumoureval']\n",
    "for dataset in datasets:\n",
    "    df = pd.read_csv(f\"{dataset}_amr/{dataset}.csv\")\n",
    "    print(dataset, df.shape[0], '\\n', df['labels'].value_counts(), end = '\\n\\n*****************************\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "antivax 3797 \n",
    " 0    2865\n",
    "1     932\n",
    "Name: labels, dtype: int64\n",
    "\n",
    "*****************************\n",
    "fake-emo 7012 \n",
    " real    3793\n",
    "fake    3219\n",
    "Name: labels, dtype: int64\n",
    "\n",
    "*****************************\n",
    "jing_gab 15330 \n",
    " NON_HATE    7665\n",
    "HATE        7665\n",
    "Name: labels, dtype: int64\n",
    "\n",
    "*****************************\n",
    "jing_reddit 4978 \n",
    " NON_HATE    2490\n",
    "HATE        2488\n",
    "Name: labels, dtype: int64\n",
    "\n",
    "*****************************\n",
    "figlang_reddit 4400 \n",
    " NOT_SARCASM    2200\n",
    "SARCASM        2200\n",
    "Name: labels, dtype: int64\n",
    "\n",
    "*****************************\n",
    "figlang_twitter 5000\n",
    "SARCASM        2500\n",
    "NOT_SARCASM    2500\n",
    "Name: labels, dtype: int64\n",
    "\n",
    "*****************************\n",
    "hasoc 712 \n",
    " NONE    414\n",
    "HOF     298\n",
    "Name: labels, dtype: int64\n",
    "\n",
    "*****************************\n",
    "pheme 6425 \n",
    " non_rumour    4023\n",
    "rumour        2402\n",
    "Name: labels, dtype: int64\n",
    "\n",
    "*****************************\n",
    "twitter15 543 \n",
    " True     276\n",
    "False    267\n",
    "Name: labels, dtype: int64\n",
    "\n",
    "*****************************\n",
    "twitter16 362 \n",
    " False    199\n",
    "True     163\n",
    "Name: labels, dtype: int64\n",
    "\n",
    "*****************************\n",
    "rumoureval 446 \n",
    " real          185\n",
    "fake          138\n",
    "unverified    123\n",
    "Name: labels, dtype: int64\n",
    "\n",
    "*****************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# finding max comments and sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/karish19471/btp/amr\n"
     ]
    }
   ],
   "source": [
    "cd btp/amr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gossipcop:- \t mean: 19.67223604692499, max: 50, min: 3, std: 18.228823638570177\n"
     ]
    }
   ],
   "source": [
    "for data in ['gossipcop']:\n",
    "    df = pd.read_csv(f\"{data}_amr/{data}.tsv\", sep =  '\\t')\n",
    "    lens = df['num_comments']\n",
    "    print(f\"{data}:- \\t mean: {np.mean(lens)}, max: {np.max(lens)}, min: {np.min(lens)}, std: {np.std(lens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amr2dgl.ipynb            \u001b[0m\u001b[01;34mfake-emo_amr\u001b[0m/         \u001b[01;34mjing_reddit_amr\u001b[0m/\n",
      "amr2hgnn_format.ipynb    \u001b[01;34mfiglang_reddit_amr\u001b[0m/   pca_embedding_8.txt\n",
      "\u001b[01;34mamr_coref\u001b[0m/               \u001b[01;34mfiglang_twitter_amr\u001b[0m/  \u001b[01;34mpheme_amr\u001b[0m/\n",
      "amr_generation.ipynb     gen_amr.py            \u001b[01;34mpolitifact_amr\u001b[0m/\n",
      "amr_preprocessing.ipynb  \u001b[01;34mgossipcop_amr\u001b[0m/        \u001b[01;34mrumoureval_amr\u001b[0m/\n",
      "\u001b[01;34mantivax_amr\u001b[0m/             \u001b[01;34mhasoc_amr\u001b[0m/            \u001b[01;34mtwitter15_amr\u001b[0m/\n",
      "data_processing.ipynb    \u001b[01;34mjing_gab_amr\u001b[0m/         \u001b[01;34mtwitter16_amr\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "politifact_amr/politifact_amr_raw/politifact15342.csv:- \t mean: 28.47342995169082, max: 50, min: 1, std: 20.560221885578564\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "file = glob.glob('politifact_amr/politifact_amr_raw/politifact*.csv')\n",
    "lens = []\n",
    "for data in file:\n",
    "    df = pd.read_csv(data)\n",
    "    lens.append(df.shape[0])\n",
    "lens = np.array(lens)\n",
    "print(f\"{data}:- \\t mean: {np.mean(lens)}, max: {np.max(lens)}, min: {np.min(lens)}, std: {np.std(lens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "antivax:- \t mean: 8.046204620462046, max: 493, min: 1, std: 31.002121354826425\n",
      "figlang_twitter:- \t mean: 3.8676, max: 20, min: 2, std: 3.218457742459888\n",
      "figlang_reddit:- \t mean: 2.4911363636363637, max: 8, min: 2, std: 0.756134655844179\n",
      "twitter16:- \t mean: 27.56145251396648, max: 486, min: 1, std: 48.697049686500854\n",
      "rumoureval:- \t mean: 17.546067415730338, max: 195, min: 1, std: 22.962221323954843\n",
      "pheme:- \t mean: 17.29544264012572, max: 345, min: 1, std: 19.983152110337084\n",
      "twitter15:- \t mean: 8.567107750472589, max: 500, min: 1, std: 24.126316439585658\n",
      "hasoc:- \t mean: 9.777272727272727, max: 272, min: 1, std: 31.973100888727878\n"
     ]
    }
   ],
   "source": [
    "for data in ['antivax', 'figlang_twitter', 'figlang_reddit', 'twitter16', 'rumoureval', 'pheme', 'twitter15', 'hasoc']:\n",
    "    df = pd.read_csv(f\"{data}_amr/{data}.csv\")\n",
    "    comments = df['comments']\n",
    "    lens = []\n",
    "    for i, j in df.iterrows():\n",
    "        try:\n",
    "            lens.append(len(j['comments'].split(\"::\")))\n",
    "        except:\n",
    "            pass\n",
    "    lens = np.array(lens)\n",
    "    print(f\"{data}:- \\t mean: {np.mean(lens)}, max: {np.max(lens)}, min: {np.min(lens)}, std: {np.std(lens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "antivax:- \t mean: 1.7368975506979194, max: 9, min: 1, std: 0.8688203132253286\n",
      "figlang_twitter:- \t mean: 2.0704, max: 10, min: 1, std: 1.2710011172300362\n",
      "figlang_reddit:- \t mean: 1.0079545454545455, max: 3, min: 1, std: 0.0913554383477113\n",
      "twitter16:- \t mean: 1.5165745856353592, max: 5, min: 1, std: 0.7403160301840561\n",
      "rumoureval:- \t mean: 1.4080717488789238, max: 8, min: 1, std: 0.7747861584105995\n",
      "pheme:- \t mean: 1.3771206225680934, max: 8, min: 1, std: 0.6576975203326774\n",
      "twitter15:- \t mean: 1.5498154981549817, max: 5, min: 1, std: 0.7968514859635492\n",
      "hasoc:- \t mean: 1.609375, max: 7, min: 1, std: 0.9733509692680231\n"
     ]
    }
   ],
   "source": [
    "for data in ['antivax', 'figlang_twitter', 'figlang_reddit', 'twitter16', 'rumoureval', 'pheme', 'twitter15', 'hasoc']:\n",
    "    df = pd.read_csv(f\"{data}_amr/{data}.csv\")\n",
    "    lens = []\n",
    "    for i, j in df.iterrows():\n",
    "        try:\n",
    "            lens.append(len(sent_tokenize(j['text'])))\n",
    "        except:\n",
    "            pass\n",
    "    lens = np.array(lens)\n",
    "    print(f\"{data}:- \\t mean: {np.mean(lens)}, max: {np.max(lens)}, min: {np.min(lens)}, std: {np.std(lens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "fake = glob.glob(\"annotation/fake/*.csv\")\n",
    "real = glob.glob(\"annotation/real/*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total annotated files:-  357\n"
     ]
    }
   ],
   "source": [
    "print(\"Total annotated files:- \", len(fake) + len(real))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "all_labels = set()\n",
    "for i in fake + real:\n",
    "    df = pd.read_csv(i)\n",
    "    df['label'] = df['label'].astype(str).apply(lambda x: x.lower())\n",
    "    all_labels = all_labels.union(set(df['label'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'-',\n",
       " 'false',\n",
       " 'igmored',\n",
       " 'ignored',\n",
       " 'nan',\n",
       " 'noies',\n",
       " 'noise',\n",
       " 'non_check_worthy',\n",
       " 'non_check_wrthy',\n",
       " 'non_checkworthy',\n",
       " 'nosie',\n",
       " 'quote',\n",
       " 'true',\n",
       " 'un_check_worthy',\n",
       " 'uncheckworthy',\n",
       " 'unverified'}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "true = ['true']\n",
    "false = ['false']\n",
    "ignored = ['igmored', 'ignored', '-', 'nan']\n",
    "noise = ['nosie', 'noies', 'noise']\n",
    "quote = ['quote']\n",
    "unverified = ['unverified']\n",
    "non_check_worthy = ['un_check_worthy','uncheckworthy', 'non_check_worthy', 'non_check_wrthy', 'non_checkworthy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136\r"
     ]
    }
   ],
   "source": [
    "lv = 0\n",
    "for i in real:\n",
    "    df = pd.read_csv(i)\n",
    "    df['label'] = df['label'].astype(str).apply(lambda x: x.lower())\n",
    "    labels = []\n",
    "    for sample in df['label']:\n",
    "        if sample in true:\n",
    "            labels.append('true')\n",
    "        elif sample in false:\n",
    "            labels.append('false')\n",
    "        elif sample in ignored:\n",
    "            labels.append('ignored')\n",
    "        elif sample in quote:\n",
    "            labels.append('quote')\n",
    "        elif sample in unverified:\n",
    "            labels.append('unverified')\n",
    "        elif sample in non_check_worthy:\n",
    "            labels.append('non_check_worthy')\n",
    "        elif sample in noise:\n",
    "            labels.append('noise')\n",
    "\n",
    "    name = i[i.rfind(\"/\")+1:]\n",
    "    df['label'] = np.array(labels)\n",
    "    dst = \"annotation_processed/real/\"\n",
    "    os.makedirs(dst, exist_ok= True)\n",
    "    df.to_csv(dst+name)\n",
    "    print(lv, end = '\\r', flush = True)\n",
    "    lv+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "fake = glob.glob(\"annotation_processed/fake/*.csv\")\n",
    "real = glob.glob(\"annotation_processed/real/*.csv\")\n",
    "all_labels = set()\n",
    "for i in fake + real:\n",
    "    prefix = i[:i.rfind(\"/\")+1]\n",
    "    suffix = i[i.rfind(\"/\")+1:]\n",
    "    os.rename(i, prefix + suffix.replace(\"-\", \"_\").split(\"_\")[0].strip()+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
