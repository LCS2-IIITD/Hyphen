{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 469,
   "id": "fab5e1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import amrlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a21ad1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']= \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816af07f",
   "metadata": {},
   "source": [
    "# new datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "776c07d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/karish19471/btp/amr\n"
     ]
    }
   ],
   "source": [
    "cd btp/amr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23bb2309",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'pheme'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4455d983",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49816faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "stog = amrlib.load_stog_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67422429",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "gid=x Start paren present but religious-group is not a new concept\n",
      "gid=x Start paren present but name is not a new concept\n",
      "gid=x Start paren present but poll-01 is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done 0/6425\tDone 33/90929\n",
      "Done 1/6425\tDone 53/90929\n",
      "Done 2/6425\tDone 57/90929\n",
      "Done 3/6425\tDone 62/90929\n",
      "Done 4/6425\tDone 78/90929\n",
      "Done 5/6425\tDone 84/90929\n",
      "Done 6/6425\tDone 94/90929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "gid=x Start paren present but yes is not a new concept\n",
      "gid=x Start paren present but yes is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done 7/6425\tDone 99/90929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "gid=x Start paren present but amr-unknown is not a new concept\n",
      "gid=x Start paren present but point-finger-06 is not a new concept\n",
      "gid=x Start paren present but dispute-01 is not a new concept\n",
      "gid=x Start paren present but have-03 is not a new concept\n",
      "gid=x Start paren present but you is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done 8/6425\tDone 149/90929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "gid=x Start paren present but along is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done 9/6425\tDone 169/90929\n",
      "Done 10/6425\tDone 170/90929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "gid=x Missing starting paren for node p/person\n",
      "gid=x Start paren present but person is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done 11/6425\tDone 190/90929\n",
      "Done 12/6425\tDone 191/90929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "gid=x Start paren present but obligate-01 is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done 13/6425\tDone 204/90929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "gid=x Start paren present but name is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done 14/6425\tDone 218/90929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "gid=x Start paren present but i is not a new concept\n",
      "gid=x Start paren present but before is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done 15/6425\tDone 233/90929\n",
      "Done 16/6425\tDone 234/90929\n",
      "Done 17/6425\tDone 244/90929\n",
      "Done 18/6425\tDone 264/90929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "gid=x Start paren present but city is not a new concept\n",
      "gid=x Start paren present but city is not a new concept\n",
      "gid=x Start paren present but person is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done 19/6425\tDone 269/90929\n",
      "Done 20/6425\tDone 270/90929\n",
      "Done 21/6425\tDone 271/90929\n",
      "Done 22/6425\tDone 280/90929\n",
      "Done 23/6425\tDone 281/90929\n",
      "Done 24/6425\tDone 295/90929\n",
      "Done 25/6425\tDone 296/90929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "gid=x Start paren present but i is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done 26/6425\tDone 306/90929\n",
      "Done 27/6425\tDone 307/90929\n",
      "Done 28/6425\tDone 325/90929\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/karish19471/btp/amr/amr_generation.ipynb Cell 8'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.21.214/home/karish19471/btp/amr/amr_generation.ipynb#ch0000006vscode-remote?line=6'>7</a>\u001b[0m ids \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(df[\u001b[39m'\u001b[39m\u001b[39mid\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.21.214/home/karish19471/btp/amr/amr_generation.ipynb#ch0000006vscode-remote?line=7'>8</a>\u001b[0m \u001b[39mfor\u001b[39;00m lv \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(ids)):\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B192.168.21.214/home/karish19471/btp/amr/amr_generation.ipynb#ch0000006vscode-remote?line=8'>9</a>\u001b[0m     graphs \u001b[39m=\u001b[39m stog\u001b[39m.\u001b[39;49mparse_sents(comments[lv])\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.21.214/home/karish19471/btp/amr/amr_generation.ipynb#ch0000006vscode-remote?line=9'>10</a>\u001b[0m     data \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.21.214/home/karish19471/btp/amr/amr_generation.ipynb#ch0000006vscode-remote?line=10'>11</a>\u001b[0m     data[\u001b[39m'\u001b[39m\u001b[39mcomment\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(comments[lv])\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///home/karish19471/.local/lib/python3.8/site-packages/torch/autograd/grad_mode.py?line=23'>24</a>\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     <a href='file:///home/karish19471/.local/lib/python3.8/site-packages/torch/autograd/grad_mode.py?line=24'>25</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     <a href='file:///home/karish19471/.local/lib/python3.8/site-packages/torch/autograd/grad_mode.py?line=25'>26</a>\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> <a href='file:///home/karish19471/.local/lib/python3.8/site-packages/torch/autograd/grad_mode.py?line=26'>27</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/amrlib/models/parse_xfm/inference.py:75\u001b[0m, in \u001b[0;36mInference.parse_sents\u001b[0;34m(self, sents, add_metadata, disable_progress)\u001b[0m\n\u001b[1;32m     <a href='file:///home/karish19471/.local/lib/python3.8/site-packages/amrlib/models/parse_xfm/inference.py?line=72'>73</a>\u001b[0m attention_mask \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mLongTensor(input_encodings[\u001b[39m'\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m'\u001b[39m])\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m     <a href='file:///home/karish19471/.local/lib/python3.8/site-packages/amrlib/models/parse_xfm/inference.py?line=73'>74</a>\u001b[0m \u001b[39m# Generate the batch ids and convert to back to tokens\u001b[39;00m\n\u001b[0;32m---> <a href='file:///home/karish19471/.local/lib/python3.8/site-packages/amrlib/models/parse_xfm/inference.py?line=74'>75</a>\u001b[0m outs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mgenerate(input_ids\u001b[39m=\u001b[39;49minput_ids, attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m     <a href='file:///home/karish19471/.local/lib/python3.8/site-packages/amrlib/models/parse_xfm/inference.py?line=75'>76</a>\u001b[0m                            max_length\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_graph_len, early_stopping\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     <a href='file:///home/karish19471/.local/lib/python3.8/site-packages/amrlib/models/parse_xfm/inference.py?line=76'>77</a>\u001b[0m                            num_beams\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_beams, num_return_sequences\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_ret_seq)\n\u001b[1;32m     <a href='file:///home/karish19471/.local/lib/python3.8/site-packages/amrlib/models/parse_xfm/inference.py?line=77'>78</a>\u001b[0m outs \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer\u001b[39m.\u001b[39mdecode(ids, skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \u001b[39mfor\u001b[39;00m ids \u001b[39min\u001b[39;00m outs]\n\u001b[1;32m     <a href='file:///home/karish19471/.local/lib/python3.8/site-packages/amrlib/models/parse_xfm/inference.py?line=78'>79</a>\u001b[0m \u001b[39m# De-sort the output token data. There are self.num_ret_seq returned for each sentence\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///home/karish19471/.local/lib/python3.8/site-packages/torch/autograd/grad_mode.py?line=23'>24</a>\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     <a href='file:///home/karish19471/.local/lib/python3.8/site-packages/torch/autograd/grad_mode.py?line=24'>25</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     <a href='file:///home/karish19471/.local/lib/python3.8/site-packages/torch/autograd/grad_mode.py?line=25'>26</a>\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> <a href='file:///home/karish19471/.local/lib/python3.8/site-packages/torch/autograd/grad_mode.py?line=26'>27</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/generation_utils.py:1044\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, input_ids, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, repetition_penalty, bad_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, **model_kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/karish19471/.local/lib/python3.8/site-packages/transformers/generation_utils.py?line=1039'>1040</a>\u001b[0m     \u001b[39m# interleave with `num_beams`\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/karish19471/.local/lib/python3.8/site-packages/transformers/generation_utils.py?line=1040'>1041</a>\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   <a href='file:///home/karish19471/.local/lib/python3.8/site-packages/transformers/generation_utils.py?line=1041'>1042</a>\u001b[0m         input_ids, expand_size\u001b[39m=\u001b[39mnum_beams, is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs\n\u001b[1;32m   <a href='file:///home/karish19471/.local/lib/python3.8/site-packages/transformers/generation_utils.py?line=1042'>1043</a>\u001b[0m     )\n\u001b[0;32m-> <a href='file:///home/karish19471/.local/lib/python3.8/site-packages/transformers/generation_utils.py?line=1043'>1044</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbeam_search(\n\u001b[1;32m   <a href='file:///home/karish19471/.local/lib/python3.8/site-packages/transformers/generation_utils.py?line=1044'>1045</a>\u001b[0m         input_ids,\n\u001b[1;32m   <a href='file:///home/karish19471/.local/lib/python3.8/site-packages/transformers/generation_utils.py?line=1045'>1046</a>\u001b[0m         beam_scorer,\n\u001b[1;32m   <a href='file:///home/karish19471/.local/lib/python3.8/site-packages/transformers/generation_utils.py?line=1046'>1047</a>\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mlogits_processor,\n\u001b[1;32m   <a href='file:///home/karish19471/.local/lib/python3.8/site-packages/transformers/generation_utils.py?line=1047'>1048</a>\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mstopping_criteria,\n\u001b[1;32m   <a href='file:///home/karish19471/.local/lib/python3.8/site-packages/transformers/generation_utils.py?line=1048'>1049</a>\u001b[0m         max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m   <a href='file:///home/karish19471/.local/lib/python3.8/site-packages/transformers/generation_utils.py?line=1049'>1050</a>\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mpad_token_id,\n\u001b[1;32m   <a href='file:///home/karish19471/.local/lib/python3.8/site-packages/transformers/generation_utils.py?line=1050'>1051</a>\u001b[0m         eos_token_id\u001b[39m=\u001b[39;49meos_token_id,\n\u001b[1;32m   <a href='file:///home/karish19471/.local/lib/python3.8/site-packages/transformers/generation_utils.py?line=1051'>1052</a>\u001b[0m         output_scores\u001b[39m=\u001b[39;49moutput_scores,\n\u001b[1;32m   <a href='file:///home/karish19471/.local/lib/python3.8/site-packages/transformers/generation_utils.py?line=1052'>1053</a>\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39;49mreturn_dict_in_generate,\n\u001b[1;32m   <a href='file:///home/karish19471/.local/lib/python3.8/site-packages/transformers/generation_utils.py?line=1053'>1054</a>\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   <a href='file:///home/karish19471/.local/lib/python3.8/site-packages/transformers/generation_utils.py?line=1054'>1055</a>\u001b[0m     )\n\u001b[1;32m   <a href='file:///home/karish19471/.local/lib/python3.8/site-packages/transformers/generation_utils.py?line=1056'>1057</a>\u001b[0m \u001b[39melif\u001b[39;00m is_beam_sample_gen_mode:\n\u001b[1;32m   <a href='file:///home/karish19471/.local/lib/python3.8/site-packages/transformers/generation_utils.py?line=1057'>1058</a>\u001b[0m     logits_warper \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_logits_warper(\n\u001b[1;32m   <a href='file:///home/karish19471/.local/lib/python3.8/site-packages/transformers/generation_utils.py?line=1058'>1059</a>\u001b[0m         top_k\u001b[39m=\u001b[39mtop_k, top_p\u001b[39m=\u001b[39mtop_p, temperature\u001b[39m=\u001b[39mtemperature, num_beams\u001b[39m=\u001b[39mnum_beams\n\u001b[1;32m   <a href='file:///home/karish19471/.local/lib/python3.8/site-packages/transformers/generation_utils.py?line=1059'>1060</a>\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/generation_utils.py:1790\u001b[0m, in \u001b[0;36mGenerationMixin.beam_search\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, **model_kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/karish19471/.local/lib/python3.8/site-packages/transformers/generation_utils.py?line=1786'>1787</a>\u001b[0m \u001b[39mif\u001b[39;00m model_kwargs[\u001b[39m\"\u001b[39m\u001b[39mpast\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   <a href='file:///home/karish19471/.local/lib/python3.8/site-packages/transformers/generation_utils.py?line=1787'>1788</a>\u001b[0m     model_kwargs[\u001b[39m\"\u001b[39m\u001b[39mpast\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reorder_cache(model_kwargs[\u001b[39m\"\u001b[39m\u001b[39mpast\u001b[39m\u001b[39m\"\u001b[39m], beam_idx)\n\u001b[0;32m-> <a href='file:///home/karish19471/.local/lib/python3.8/site-packages/transformers/generation_utils.py?line=1789'>1790</a>\u001b[0m \u001b[39mif\u001b[39;00m beam_scorer\u001b[39m.\u001b[39mis_done:\n\u001b[1;32m   <a href='file:///home/karish19471/.local/lib/python3.8/site-packages/transformers/generation_utils.py?line=1790'>1791</a>\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/karish19471/.local/lib/python3.8/site-packages/transformers/generation_utils.py?line=1792'>1793</a>\u001b[0m \u001b[39mif\u001b[39;00m stopping_criteria(input_ids, scores):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(f'{dataset}_amr/{dataset}.csv')\n",
    "df['comments'] = df['comments'].fillna(\" \")\n",
    "total = df.shape[0]\n",
    "comments = [df.iloc[i]['comments'].split('::')[:50] for i in range(df.shape[0])]\n",
    "num_comments = np.sum(np.array([len(i) for i in comments]))\n",
    "done_comments = 0\n",
    "ids = list(df['id'])\n",
    "for lv in range(len(ids)):\n",
    "    graphs = stog.parse_sents(comments[lv])\n",
    "    data = pd.DataFrame()\n",
    "    data['comment'] = np.array(comments[lv])\n",
    "    data['amr'] = np.array(graphs)\n",
    "    dest_dir = f\"{dataset}_amr/{dataset}_amr_csv/\"\n",
    "    os.makedirs(dest_dir, exist_ok = True)\n",
    "    data.to_csv(dest_dir+ ids[lv]+'.csv', index = False)\n",
    "    done_comments+=len(comments[lv])\n",
    "    print(f\"Done {lv}/{total}\\tDone {done_comments}/{num_comments}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9cd7857",
   "metadata": {},
   "source": [
    "# Politifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b978af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #read the dataset\n",
    "# df = pd.read_csv('static/politifact_comment_no_ignore.tsv', sep = '\\t')\n",
    "# comments = [df.iloc[i]['comment'].split('::')[:50] for i in range(df.shape[0])]\n",
    "# num_comments = np.array([len(i) for i in comments])\n",
    "# stog = amrlib.load_stog_model()\n",
    "# ids = list(df['id'])\n",
    "# for lv in range(len(ids)):\n",
    "#     graphs = stog.parse_sents(comments[lv])\n",
    "#     data = pd.DataFrame()\n",
    "#     data['comment'] = np.array(comments[lv])\n",
    "#     data['amr'] = np.array(graphs)\n",
    "#     data.to_csv(ids[lv]+'.csv', index = False)\n",
    "#     print(\"Done\", lv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64f1c97",
   "metadata": {},
   "source": [
    "# Gossipcop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b966199f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# import glob\n",
    "# import numpy as np\n",
    "# import preprocessor as p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318c4e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gossipcop = glob.glob(\"gossipcop_amr/gossipcop_amr_csv/gossipcop-*/\")\n",
    "# len_comments = []\n",
    "# df = pd.DataFrame()\n",
    "# id_list, comments_list = [], []\n",
    "# for lv, news in enumerate(gossipcop):\n",
    "#     comments = glob.glob(f'{news}*.json')\n",
    "#     all_comments= []\n",
    "#     for comm in comments: \n",
    "#         user_comment = json.load(open(comm, 'rb'))\n",
    "#         for i in user_comment['data']:\n",
    "#             all_comments.append(p.clean(i['text']))\n",
    "#     if len(all_comments)>0:#considering only news articles with atleast one comment\n",
    "#         processed = \"::\".join(all_comments)\n",
    "#         comments_list.append(processed)\n",
    "#         id_list.append(news[:-1][news[:-1].rfind(\"/\")+1:])\n",
    "#     print(lv, end = '\\r', flush = True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ceae51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['id'] =  id_list\n",
    "# df['comments'] = comments_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f5d7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv(\"gossipcop_amr/gossipcop_amr.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd3a090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #read the dataset\n",
    "# df = pd.read_csv('gossipcop_amr/gossipcop_amr.csv')\n",
    "# df = df.dropna()\n",
    "# comments = [df.iloc[i]['comments'].split('::')[:50] for i in range(df.shape[0])]\n",
    "# num_comments = np.array([len(i) for i in comments])\n",
    "# stog = amrlib.load_stog_model()\n",
    "# ids = list(df['id'])\n",
    "# total = 0\n",
    "# for lv in range(len(ids)):\n",
    "#     total+=len(comments[lv])\n",
    "#     graphs = stog.parse_sents(comments[lv])\n",
    "#     data = pd.DataFrame()\n",
    "#     data['comment'] = np.array(comments[lv])\n",
    "#     data['amr'] = np.array(graphs)\n",
    "#     data.to_csv(f\"gossipcop_amr/gossipcop_amr_csv/{ids[lv]}.csv\", index = False)\n",
    "#     print(total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166cac6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len_comments = np.array(len_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a08927a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.sum(len_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b59d26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gossipcop = glob.glob(\"gossipcop_amr/gossipcop_amr_csv/gossipcop-*/\")\n",
    "# len_comments = []\n",
    "# for news in gossipcop:\n",
    "#     comments = glob.glob(f'{news}*.json')\n",
    "#     for comm in comments:\n",
    "#         user_comment = json.load(open(comm, 'rb'))\n",
    "#         print(user_comment)\n",
    "#         break\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90a0acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len_comments = np.asarray(len_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54da321a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(len_comments[len_comments >= 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a163c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#9:26 start time#read the dataset\n",
    "# df = pd.read_csv('static/politifact_comment_no_ignore.tsv', sep = '\\t')\n",
    "# comments = [df.iloc[i]['comment'].split('::')[:50] for i in range(df.shape[0])]\n",
    "# num_comments = np.array([len(i) for i in comments])\n",
    "# stog = amrlib.load_stog_model()\n",
    "# ids = list(df['id'])\n",
    "# for lv in range(len(ids)):\n",
    "#     graphs = stog.parse_sents(comments[lv])\n",
    "#     data = pd.DataFrame()\n",
    "#     data['comment'] = np.array(comments[lv])\n",
    "#     data['amr'] = np.array(graphs)\n",
    "#     data.to_csv(ids[lv]+'.csv', index = False)\n",
    "#     print(\"Done\", lv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
